{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzfwGshjDDfT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckinzthompson/removecysteines/blob/main/remove_cysteines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8hKKXREDJ8D"
      },
      "source": [
        "## RemoveCysteines v0.1.1: Use ESM2 to remove cysteines from protein sequences\n",
        "\n",
        "[Evolutionary Scale Modeling (ESM)](https://github.com/facebookresearch/esm) is a masked-large language model trained on protein sequences. The science is described in [Lin et al.](https://www.science.org/doi/10.1126/science.ade2574). Because ESM has information containing sequence conservation, we've used it here to design cys-less variants of proteins that should (ideally) maintain function. Previously, researchers would manually create a multiple sequence alignment showing the conservation of the cysteines in the wild type sequence, and then pick other amino acids that are represented at those positions.\n",
        "\n",
        "Our approach using ESM uses the following steps:\n",
        "1. Find the second-best mutation to make (i.e., whichever 'looks' most like a cysteine)\n",
        "2. Make all possible point mutations, and select the best (i.e., whichever makes the entire sequence look most like a real protein)\n",
        "3. Repeat step 2 until the sequence is optimized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yluTa4yQ_d8V"
      },
      "outputs": [],
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "\n",
        "#@markdown\tEnter wild type (amino acid) sequence\n",
        "wt_sequence = 'ASNFTQFVLVDNGGTGDVTVAPSNFANGVAEWISSNSRSQAYKVTCSVRQSSAQKRKYTIKVEVPKVATQTVGGVELPVAAWRSYLNMELTIPIFATNSDCELIVKAMQGLLKDGNPIPSAIAANSGLY' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown\tMaximum number of polishing steps\n",
        "n_rounds = 20 # @param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "# @markdown\tESM model to use\n",
        "ESM_model_name = \"esm2_t33_650M_UR50D\" # @param [\"esm2_t6_8M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t33_650M_UR50D\", \"esm2_t36_3B_UR50D\", \"esm2_t48_15B_UR50D\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lN_qIbOc-5qo"
      },
      "outputs": [],
      "source": [
        "#@title Install ESM and Download Model\n",
        "\n",
        "try:\n",
        "\timport esm\n",
        "except:\n",
        "\timport os\n",
        "\tos.system('pip install fair-esm')\n",
        "\timport esm\n",
        "\n",
        "## Download model\n",
        "if ESM_model_name == \"esm2_t6_8M_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t33_650M_UR50D\n",
        "elif ESM_model_name == \"esm2_t12_35M_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t12_35M_UR50D\n",
        "elif ESM_model_name == \"esm2_t30_150M_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t30_150M_UR50D\n",
        "elif ESM_model_name == \"esm2_t33_650M_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t33_650M_UR50D\n",
        "elif ESM_model_name == \"esm2_t36_3B_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t36_3B_UR50D\n",
        "elif ESM_model_name == \"esm2_t48_15B_UR50D\":\n",
        "\t_loader = esm.pretrained.esm2_t48_15B_UR50D\n",
        "\n",
        "model, alphabet = _loader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0BiSy3s3_R59"
      },
      "outputs": [],
      "source": [
        "#@title Run Code\n",
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tdevice = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "\tdevice = 'mps'\n",
        "else:\n",
        "\tdevice = 'cpu'\n",
        "\n",
        "show_pca = True\n",
        "\n",
        "letters = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\n",
        "letterids = np.array([4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
        "\n",
        "def _embed_sequences(data,model,batch_converter):\n",
        "\tbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "\t## note: encoded sequence has a start and end position, so length is two longer than sequence. removing\n",
        "\tbatch_tokens = batch_tokens.to(device)\n",
        "\twith torch.no_grad():\n",
        "\t\tresults = model(batch_tokens, repr_layers=[model.num_layers,], return_contacts=False)\n",
        "\n",
        "\treps = results['representations'][model.num_layers].cpu().numpy()\n",
        "\tlogits = results['logits'].cpu().numpy()\n",
        "\n",
        "\tif device == 'mps':\n",
        "\t\ttorch.mps.empty_cache()\n",
        "\n",
        "\treturn reps,logits\n",
        "\n",
        "def embed_sequences(data,model,batch_converter):\n",
        "\t'''\n",
        "\tNote: on MPS it's essential to iterate rather than batch, b/c of memory pressure issues. I noticed completely wrong values popping up b/c of swapping (I think).\n",
        "\texternal validation here: https://huggingface.co/docs/diffusers/en/optimization/mps\n",
        "\t'''\n",
        "\n",
        "\tif device == 'mps':\n",
        "\t\treps,logits = _embed_sequences([data[0],],model,batch_converter)\n",
        "\n",
        "\t\tfor i in range(1,len(data)):\n",
        "\t\t\t_reps,_logits = _embed_sequences([data[i],],model,batch_converter)\n",
        "\t\t\treps = np.concatenate((reps,_reps),axis=0)\n",
        "\t\t\tlogits = np.concatenate((logits,_logits),axis=0)\n",
        "\telse:\n",
        "\t\treps,logits = _embed_sequences(data,model,batch_converter)\n",
        "\treturn reps,logits\n",
        "\n",
        "def generate_pointmutants(sequence,index):\n",
        "\tdata = []\n",
        "\tfor letter in letters:\n",
        "\t\tmutated_sequence = sequence[:index] + letter + sequence[index+1:]\n",
        "\t\tdata.append(('%d%s'%(index,letter),mutated_sequence))\n",
        "\treturn data\n",
        "\n",
        "def calc_pseudoperplexity(logits,seq):\n",
        "\t#### eqn 4\n",
        "\t#### logits (seq,latent)\n",
        "\t#### sequence (seq)\n",
        "\n",
        "\t#### calculate probabilities\n",
        "\tprobs = np.exp(logits)\n",
        "\tprobs /= np.sum(probs,axis=1)[:,None]\n",
        "\n",
        "\t## decode sequence\n",
        "\tseq_ids = np.array([letterids[letters.index(seq[i])] for i in range(len(seq))])\n",
        "\tnlp = -np.log(probs[1:-1,seq_ids]) ## remove CLS and EOS tokens.\n",
        "\n",
        "\t## calculate pseudoperplexity\n",
        "\tpppl = np.exp(np.mean(nlp))\n",
        "\treturn pppl\n",
        "\n",
        "def calc_pseudoperplexities(logits,data):\n",
        "\tperp = np.array([calc_pseudoperplexity(logits[i],data[i][1]) for i in range(len(data))])\n",
        "\treturn perp\n",
        "\n",
        "def calc_given_best(sequence,model,batch_converter,indices):\n",
        "\tdata = [(0,sequence)]\n",
        "\treps, logits = embed_sequences(data,model,batch_converter)\n",
        "\n",
        "\tout = []\n",
        "\tfor index in indices:\n",
        "\t\tp = softmax(logits[0,index+1,letterids])\n",
        "\t\tp[letters.index(sequence[index])] = 0.\n",
        "\t\tp /= p.sum()\n",
        "\t\tout.append([sequence[index],index,letters[p.argmax()]])\n",
        "\treturn out\n",
        "\n",
        "__DEBUG__ = False\n",
        "\n",
        "#### Load ESM-2 model\n",
        "# model, alphabet = _loader()\n",
        "print('ESM: %s'%(ESM_model_name))\n",
        "\n",
        "#### Find GPU type\n",
        "print('Using Device:',device)\n",
        "print('----------')\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()\t# disables dropout for deterministic results\n",
        "model = model.to(device) # put onto the gpu\n",
        "\n",
        "#### Clean input sequence\n",
        "wt_sequence = ''.join([si for si in wt_sequence if si in letters])\n",
        "\n",
        "#### Reporting Statistics\n",
        "print('WT Sequence: %s'%(wt_sequence))\n",
        "print('Length: %d'%(len(wt_sequence)))\n",
        "\n",
        "ncys = wt_sequence.count('C')\n",
        "print('Num. Cys: %d'%(ncys))\n",
        "if ncys == 0:\n",
        "\tprint('No Cys to remove. Finished!')\n",
        "\tsys.exit(0)\n",
        "\n",
        "print('Cys locations:',*[index for index in range(len(wt_sequence)) if wt_sequence[index] == 'C'])\n",
        "print('\\n---------- Optimization ----------')\n",
        "reps,logits = embed_sequences([('wt',wt_sequence),],model,batch_converter)\n",
        "wt_pp = calc_pseudoperplexity(logits[0],wt_sequence)\n",
        "print('0. WT Perplexity: %.2f'%(wt_pp))\n",
        "\n",
        "## Design\n",
        "mut_sequence = ''.join(list(wt_sequence)) ## make a deep copy\n",
        "indices = np.array([index for index in range(len(mut_sequence)) if mut_sequence[index] == 'C'])\n",
        "\n",
        "#### Step 1. Remove all C using the best (unmasked) alternative\n",
        "mutations = calc_given_best(mut_sequence,model,batch_converter,indices)\n",
        "for mutation in mutations:\n",
        "\torig,ind,repl = mutation\n",
        "\tmut_sequence = mut_sequence[:ind] + repl + mut_sequence[ind+1:]\n",
        "\n",
        "reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "mut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "print('1. Initial MUT perplexity: %.2f'%(mut_pp))\n",
        "for mutation in mutations:\n",
        "\tprint('\\tC%d%s'%(mutation[1],mutation[2]))\n",
        "\n",
        "if __DEBUG__:\n",
        "\tprint(mut_sequence)\n",
        "\n",
        "#### Step 2. Scan all point changes to maximize perplexity\n",
        "cls = {}\n",
        "for iter in range(n_rounds):\n",
        "\t## get starting point\n",
        "\treps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "\tmut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "\tbest = [-1,mut_pp,-1]\n",
        "\n",
        "\tt0 = time.time()\n",
        "\tfor index in indices:\n",
        "\t\tdata = generate_pointmutants(mut_sequence,index)\n",
        "\t\tdata = data[:-1] # no C\n",
        "\t\tt0 = time.time()\n",
        "\t\treps,logits = embed_sequences(data,model,batch_converter)\n",
        "\t\tt1 = time.time()\n",
        "\t\tpp = calc_pseudoperplexities(logits,data)\n",
        "\t\tif __DEBUG__:\n",
        "\t\t\tprint(iter,index,t1-t0,pp.argmax(),pp.max(),best)\n",
        "\t\tif pp.max() > best[1]:\n",
        "\t\t\tbest = [index,pp.max(),pp.argmax()]\n",
        "\n",
        "\t\tif show_pca:\n",
        "\t\t\tfor i in range(len(data)):\n",
        "\t\t\t\tif not data[i][1] in cls:\n",
        "\t\t\t\t\tcls[data[i][1]] = reps[i,0].copy()\n",
        "\n",
        "\tprint('2.%d Polish MUT perplexity %.2f'%(iter,best[1]))\n",
        "\t# print('\\tTime:',t1-t0,(t1-t0)/(19*len(indices)))\n",
        "\tif __DEBUG__:\n",
        "\t\tprint('**',best,best[0] != -1,mut_sequence)\n",
        "\tif \tbest[0] != -1:\n",
        "\t\tprint('\\tC%d%s'%(best[0],letters[best[2]]))\n",
        "\t\tmut_sequence = mut_sequence[:best[0]] + letters[best[2]] + mut_sequence[best[0]+1:]\n",
        "\telse:\n",
        "\t\tprint('\\tNo better change')\n",
        "\t\tbreak\n",
        "\n",
        "## Step 3. Finish up\n",
        "print('\\n---------- Final ----------')\n",
        "print('MUT Sequence: %s'%(mut_sequence))\n",
        "reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "mut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "print('MUT Perplexity: %.2f'%(mut_pp))\n",
        "print('Mutations:')\n",
        "for index in indices:\n",
        "\tprint('\\t%s%d%s'%(wt_sequence[index],index,mut_sequence[index]))\n",
        "\n",
        "\n",
        "### Step 4. Analysis\n",
        "if show_pca:\n",
        "\tq = np.array([cls[k] for k in cls.keys()])\n",
        "\timport matplotlib.pyplot as plt\n",
        "\tfrom sklearn.decomposition import PCA\n",
        "\tpca = PCA(n_components=2)\n",
        "\tw = pca.fit_transform(q)\n",
        "\tplt.plot(w[:,0],w[:,1],'o',color='gray',label='Point mutants')\n",
        "\n",
        "\treps,logits = embed_sequences([('wt',wt_sequence),],model,batch_converter)\n",
        "\tww = pca.transform(reps[0,0][None,:])[0]\n",
        "\tplt.plot(ww[0],ww[1],'o',color='tab:blue',label='WT')\n",
        "\n",
        "\treps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "\tww = pca.transform(reps[0,0][None,:])[0]\n",
        "\tplt.plot(ww[0],ww[1],'o',color='tab:red',label='Final MUT')\n",
        "\n",
        "\tplt.xlabel('PCA1')\n",
        "\tplt.ylabel('PCA2')\n",
        "\tplt.legend()\n",
        "\tplt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
