{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/ckinzthompson/removecysteines/blob/main/remove_cysteines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ZzfwGshjDDfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RemoveCysteines v0.1: Use ESM2 to remove cysteines from protein sequences\n",
        "\n",
        "[Evolutionary Scale Modeling (ESM)](https://github.com/facebookresearch/esm) is a masked-large language model trained on protein sequences. The science is described in [Lin et al.](https://www.science.org/doi/10.1126/science.ade2574). Because ESM has information containing sequence conservation, we've used it here to design cys-less variants of proteins that should (ideally) maintain function. Previously, researchers would manually create a multiple sequence alignment showing the conservation of the cysteines in the wild type sequence, and then pick other amino acids that are represented at those positions.\n",
        "\n",
        "Our approach using ESM uses the following steps:\n",
        "1. Find the second-best mutation to make (i.e., whichever 'looks' most like a cysteine)\n",
        "2. Make all possible point mutations, and select the best (i.e., whichever makes the entire sequence look most like a real protein)\n",
        "3. Repeat step 2 until the sequence is optimized.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8hKKXREDJ8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "\n",
        "#@markdown  Enter wild type (amino acid) sequence\n",
        "wt_sequence = 'MHCNFIFMIYFLCLFYLIYLTNVVSLKKNFFINNVGKLDTCLTPRVGGLNKRKLSLCDKK GQLIKRFILKDNNLKNGIIKKKKENDVIEMNGIVEECLANTNFVVSIQNGEKFLCFISGK LRVNKVKINLGDTVKIQIHKLNFEQRRGKIVYRYLQQTPMKRKR' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown  Maximum number of polishing steps\n",
        "n_rounds = 20 # @param {type:\"slider\", min:0, max:100, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yluTa4yQ_d8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lN_qIbOc-5qo"
      },
      "outputs": [],
      "source": [
        "#@title Install ESM and Download Model\n",
        "\n",
        "import os\n",
        "os.system('pip install fair-esm')\n",
        "import esm\n",
        "## Download model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Code\n",
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tdevice = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "\tdevice = 'mps'\n",
        "else:\n",
        "\tdevice = 'cpu'\n",
        "\n",
        "show_pca = True\n",
        "\n",
        "letters = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\n",
        "letterids = np.array([4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
        "\n",
        "wt_sequence = ''.join([si for si in wt_sequence if si in letters])\n",
        "\n",
        "\n",
        "def _embed_sequences(data,model,batch_converter):\n",
        "\tbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "\t## note: encoded sequence has a start and end position, so length is two longer than sequence. removing\n",
        "\tbatch_tokens = batch_tokens.to(device)\n",
        "\twith torch.no_grad():\n",
        "\t\tresults = model(batch_tokens, repr_layers=[model.num_layers,], return_contacts=False)\n",
        "\n",
        "\treps = results['representations'][model.num_layers].cpu().numpy()\n",
        "\tlogits = results['logits'].cpu().numpy()\n",
        "\n",
        "\tif device == 'mps':\n",
        "\t\ttorch.mps.empty_cache()\n",
        "\n",
        "\treturn reps,logits\n",
        "\n",
        "def embed_sequences(data,model,batch_converter):\n",
        "\t'''\n",
        "\tNote: on MPS it's essential to iterate rather than batch, b/c of memory pressure issues. I noticed completely wrong values popping up b/c of swapping (I think).\n",
        "\texternal validation here: https://huggingface.co/docs/diffusers/en/optimization/mps\n",
        "\t'''\n",
        "\n",
        "\tif device == 'mps':\n",
        "\t\treps,logits = _embed_sequences([data[0],],model,batch_converter)\n",
        "\n",
        "\t\tfor i in range(1,len(data)):\n",
        "\t\t\t_reps,_logits = _embed_sequences([data[i],],model,batch_converter)\n",
        "\t\t\treps = np.concatenate((reps,_reps),axis=0)\n",
        "\t\t\tlogits = np.concatenate((logits,_logits),axis=0)\n",
        "\telse:\n",
        "\t\treps,logits = _embed_sequences(data,model,batch_converter)\n",
        "\treturn reps,logits\n",
        "\n",
        "def generate_pointmutants(sequence,index):\n",
        "\tdata = []\n",
        "\tfor letter in letters:\n",
        "\t\tmutated_sequence = sequence[:index] + letter + sequence[index+1:]\n",
        "\t\tdata.append(('%d%s'%(index,letter),mutated_sequence))\n",
        "\treturn data\n",
        "\n",
        "def calc_pseudoperplexity(logits,seq):\n",
        "\t#### eqn 4\n",
        "\t#### logits (seq,latent)\n",
        "\t#### sequence (seq)\n",
        "\n",
        "\t#### calculate probabilities\n",
        "\tprobs = np.exp(logits)\n",
        "\tprobs /= np.sum(probs,axis=1)[:,None]\n",
        "\n",
        "\t## decode sequence\n",
        "\tseq_ids = np.array([letterids[letters.index(seq[i])] for i in range(len(seq))])\n",
        "\tnlp = -np.log(probs[1:-1,seq_ids]) ## remove CLS and EOS tokens.\n",
        "\n",
        "\t## calculate pseudoperplexity\n",
        "\tpppl = np.exp(np.mean(nlp))\n",
        "\treturn pppl\n",
        "\n",
        "def calc_pseudoperplexities(logits,data):\n",
        "\tperp = np.array([calc_pseudoperplexity(logits[i],data[i][1]) for i in range(len(data))])\n",
        "\treturn perp\n",
        "\n",
        "def calc_given_best(sequence,model,batch_converter,indices):\n",
        "\tfrom scipy.special import softmax\n",
        "\n",
        "\tdata = [(0,sequence)]\n",
        "\treps, logits = embed_sequences(data,model,batch_converter)\n",
        "\n",
        "\tout = []\n",
        "\tfor index in indices:\n",
        "\t\tp = softmax(logits[0,index+1,letterids])\n",
        "\t\tp[letters.index(sequence[index])] = 0.\n",
        "\t\tp /= p.sum()\n",
        "\t\tout.append([sequence[index],index,letters[p.argmax()]])\n",
        "\treturn out\n",
        "\n",
        "\n",
        "#### Load ESM-2 model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "print('ESM2-t%s'%(model.num_layers))\n",
        "\n",
        "#### Find GPU type\n",
        "\n",
        "print('Using Device:',device)\n",
        "print('----------')\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "model = model.to(device) # put onto the gpu\n",
        "\n",
        "# ## Exclude certain letters from consideration\n",
        "# letters = [ll for ll in list(alphabet.tok_to_idx.keys()) if ll.isalpha()]\n",
        "# letters = [ll for ll in letters if ll not in ['B','J','O','U','X','Z']]\n",
        "# letterids = np.array([alphabet.tok_to_idx[ll] for ll in letters])\n",
        "\n",
        "#### Reporting Statistics\n",
        "print('WT Sequence: %s'%(wt_sequence))\n",
        "print('Length: %d'%(len(wt_sequence)))\n",
        "\n",
        "ncys = wt_sequence.count('C')\n",
        "print('Num. Cys: %d'%(ncys))\n",
        "if ncys == 0:\n",
        "  print('No Cys to remove. Finished!')\n",
        "  sys.exit(0)\n",
        "\n",
        "print('Cys locations:',*[index for index in range(len(wt_sequence)) if wt_sequence[index] == 'C'])\n",
        "print('\\n---------- Optimization ----------')\n",
        "reps,logits = embed_sequences([('wt',wt_sequence),],model,batch_converter)\n",
        "wt_pp = calc_pseudoperplexity(logits[0],wt_sequence)\n",
        "print('0. WT Perplexity: %.2f'%(wt_pp))\n",
        "\n",
        "## Design\n",
        "mut_sequence = ''.join(list(wt_sequence)) ## make a deep copy\n",
        "indices = np.array([index for index in range(len(mut_sequence)) if mut_sequence[index] == 'C'])\n",
        "\n",
        "#### Step 1. Remove all C using the best (unmasked) alternative\n",
        "mutations = calc_given_best(mut_sequence,model,batch_converter,indices)\n",
        "for mutation in mutations:\n",
        "  orig,ind,repl = mutation\n",
        "  mut_sequence = mut_sequence[:ind] + repl + mut_sequence[ind+1:]\n",
        "\n",
        "reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "mut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "print('1. Initial MUT perplexity: %.2f'%(mut_pp))\n",
        "for mutation in mutations:\n",
        "  print('\\tC%d%s'%(mutation[1],mutation[2]))\n",
        "\n",
        "#### Step 2. Scan all point changes to maximize perplexity\n",
        "cls = {}\n",
        "for iter in range(n_rounds):\n",
        "  ## get starting point\n",
        "  reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "  mut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "  best = [-1,mut_pp,-1]\n",
        "\n",
        "  t0 = time.time()\n",
        "  for index in indices:\n",
        "    data = generate_pointmutants(mut_sequence,index)\n",
        "    data = data[:-1] # no C\n",
        "    t0 = time.time()\n",
        "    reps,logits = embed_sequences(data,model,batch_converter)\n",
        "    t1 = time.time()\n",
        "    pp = calc_pseudoperplexities(logits,data)\n",
        "    if pp.max() > best[1]:\n",
        "      best = [index,pp.max(),pp.argmax()]\n",
        "\n",
        "    if show_pca:\n",
        "      for i in range(len(data)):\n",
        "        if not data[i][1] in cls:\n",
        "          cls[data[i][1]] = reps[i,0].copy()\n",
        "\n",
        "  print('2.%d Polish MUT perplexity %.2f'%(iter,best[1]))\n",
        "  # print('\\tTime:',t1-t0,(t1-t0)/(19*len(indices)))\n",
        "  if \tbest[0] != -1:\n",
        "    print('\\tC%d%s'%(best[0],letters[best[2]]))\n",
        "    mut_sequence = mut_sequence[:best[0]] + letters[best[2]] + mut_sequence[best[0]+1:]\n",
        "  else:\n",
        "    print('\\tNo better change')\n",
        "    break\n",
        "\n",
        "## Step 3. Finish up\n",
        "print('\\n---------- Final ----------')\n",
        "print('MUT Sequence: %s'%(mut_sequence))\n",
        "reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "mut_pp = calc_pseudoperplexity(logits[0],mut_sequence)\n",
        "print('MUT Perplexity: %.2f'%(mut_pp))\n",
        "print('Mutations:')\n",
        "for index in indices:\n",
        "  print('\\t%s%d%s'%(wt_sequence[index],index,mut_sequence[index]))\n",
        "\n",
        "\n",
        "### Step 4. Analysis\n",
        "if show_pca:\n",
        "  q = np.array([cls[k] for k in cls.keys()])\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=2)\n",
        "  w = pca.fit_transform(q)\n",
        "  plt.plot(w[:,0],w[:,1],'o',color='gray',label='Point mutants')\n",
        "\n",
        "  reps,logits = embed_sequences([('wt',wt_sequence),],model,batch_converter)\n",
        "  ww = pca.transform(reps[0,0][None,:])[0]\n",
        "  plt.plot(ww[0],ww[1],'o',color='tab:blue',label='WT')\n",
        "\n",
        "  reps,logits = embed_sequences([('mut',mut_sequence),],model,batch_converter)\n",
        "  ww = pca.transform(reps[0,0][None,:])[0]\n",
        "  plt.plot(ww[0],ww[1],'o',color='tab:red',label='Final MUT')\n",
        "\n",
        "  plt.xlabel('PCA1')\n",
        "  plt.ylabel('PCA2')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0BiSy3s3_R59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}